Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_act.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'activation', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'c4', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.9274594783782959
Layers [3] pruned by activation method,14.17125,0.34880995750427246
"Layers [3, 5] pruned by activation method",15.3376953125,0.31757640838623047
"Layers [3, 5, 4] pruned by activation method",12.9905078125,0.3039107322692871
"Layers [3, 5, 4, 1] pruned by activation method",60.850390625,0.2927980422973633
"Layers [3, 5, 4, 1, 2, 9] pruned by activation method",124.3859375,0.27005696296691895
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_act.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-dataset-oscarvi', 'repeat': 1, 'eval_base': True, 'pruning_method': 'activation', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,37.37957763671875,0.8456037044525146
Layers [3] pruned by activation method,48.9027099609375,0.31163644790649414
"Layers [3, 5] pruned by activation method",80.140869140625,0.30308103561401367
"Layers [3, 5, 1] pruned by activation method",398.0732421875,0.2900419235229492
"Layers [3, 5, 1, 4] pruned by activation method",492.05419921875,0.2801632881164551
"Layers [3, 5, 1, 4, 2, 6] pruned by activation method",4219.966796875,0.2622089385986328
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_act.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'activation', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.8725070953369141
Layers [3] pruned by activation method,14.17125,0.34343719482421875
"Layers [3, 5] pruned by activation method",15.3376953125,0.3148832321166992
"Layers [3, 5, 1] pruned by activation method",54.278828125,0.3057403564453125
"Layers [3, 5, 1, 4] pruned by activation method",60.850390625,0.29567956924438477
"Layers [3, 5, 1, 4, 2, 6] pruned by activation method",1081.90875,0.27292585372924805
