Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_comb.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'combine', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'c4', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.8482711315155029
Layers [12] pruned by combine method,12.7107421875,0.341778039932251
"Layers [12, 28] pruned by combine method",13.1832421875,0.313570499420166
"Layers [12, 28, 14] pruned by combine method",13.602421875,0.30199193954467773
"Layers [12, 28, 14, 29] pruned by combine method",14.6825390625,0.2909078598022461
"Layers [12, 28, 14, 29, 27, 26] pruned by combine method",18.362890625,0.27155613899230957
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_comb.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-dataset-oscarvi', 'repeat': 1, 'eval_base': True, 'pruning_method': 'combine', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,37.37957763671875,0.8509807586669922
Layers [12] pruned by combine method,41.57391357421875,0.3109719753265381
"Layers [12, 28] pruned by combine method",48.23956298828125,0.3019847869873047
"Layers [12, 28, 14] pruned by combine method",52.67840576171875,0.2953462600708008
"Layers [12, 28, 14, 29] pruned by combine method",56.353515625,0.2819681167602539
"Layers [12, 28, 14, 29, 27, 26] pruned by combine method",74.5098876953125,0.26349973678588867
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_comb.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'combine', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.9071762561798096
Layers [12] pruned by combine method,12.7107421875,0.3418722152709961
"Layers [12, 28] pruned by combine method",13.1832421875,0.31368327140808105
"Layers [12, 28, 14] pruned by combine method",13.602421875,0.30155038833618164
"Layers [12, 28, 14, 29] pruned by combine method",14.6825390625,0.29111170768737793
"Layers [12, 28, 14, 29, 27, 26] pruned by combine method",18.362890625,0.2718348503112793
