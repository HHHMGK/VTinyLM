Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'c4', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.9167625904083252
Layers [12] pruned by magnitude method,12.7107421875,0.3245210647583008
"Layers [12, 28] pruned by magnitude method",13.1832421875,0.31366872787475586
"Layers [12, 28, 14] pruned by magnitude method",13.602421875,0.30150818824768066
"Layers [12, 28, 14, 29] pruned by magnitude method",14.6825390625,0.2916126251220703
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",18.362890625,0.27059364318847656
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-dataset-oscarvi', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,37.37957763671875,0.8500502109527588
Layers [12] pruned by magnitude method,41.57391357421875,0.313382625579834
"Layers [12, 28] pruned by magnitude method",48.23956298828125,0.3012564182281494
"Layers [12, 28, 14] pruned by magnitude method",52.67840576171875,0.29174208641052246
"Layers [12, 28, 14, 29] pruned by magnitude method",56.353515625,0.28560447692871094
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",74.5098876953125,0.25829052925109863
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,0.9414136409759521
Layers [12] pruned by magnitude method,12.7107421875,0.3254990577697754
"Layers [12, 28] pruned by magnitude method",13.1832421875,0.31375932693481445
"Layers [12, 28, 14] pruned by magnitude method",13.602421875,0.30387330055236816
"Layers [12, 28, 14, 29] pruned by magnitude method",14.6825390625,0.2913661003112793
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",18.362890625,0.2707092761993408
