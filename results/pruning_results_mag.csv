Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'c4', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,1.098818302154541
Layers [12] pruned by magnitude method,12.7107421875,0.32874321937561035
"Layers [12, 28] pruned by magnitude method",13.1832421875,0.3171536922454834
"Layers [12, 28, 14] pruned by magnitude method",13.602421875,0.30676984786987305
"Layers [12, 28, 14, 29] pruned by magnitude method",14.6825390625,0.2971210479736328
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",18.362890625,0.2762129306793213
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-dataset-vnnews', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'c4', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,21.993011474609375,0.7808408737182617
Layers [12] pruned by magnitude method,22.515533447265625,0.24809956550598145
"Layers [12, 28] pruned by magnitude method",23.6959228515625,0.24232697486877441
"Layers [12, 28, 14] pruned by magnitude method",25.20904541015625,0.23202157020568848
"Layers [12, 28, 14, 29] pruned by magnitude method",28.522705078125,0.22327566146850586
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",40.64422607421875,0.20799040794372559
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-essay-vn', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,12.650546875,1.1064388751983643
Layers [12] pruned by magnitude method,12.7107421875,0.3281416893005371
"Layers [12, 28] pruned by magnitude method",13.1832421875,0.31542491912841797
"Layers [12, 28, 14] pruned by magnitude method",13.602421875,0.30799031257629395
"Layers [12, 28, 14, 29] pruned by magnitude method",14.6825390625,0.2960076332092285
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",18.362890625,0.27559447288513184
Modification,Perplexity_mean,Time_mean
"{'run_mode': 'prune', 'config': 'config.json', 'base_model': 'vinai/PhoGPT-4B-Chat', 'output': 'results/pruning_results_mag.csv', 'instructive_prompt': False, 'measure_time': True, 'output_console': True, 'bnb': 'none', 'load_peft_path': None, 'model_path': '', 'dataset_path': '', 'block_size': 1024, 'precision': 'fp16', 'eval_after_train': None, 'save_full_model': None, 'save_path': './trained_model', 'benchmark': 'perplexity-dataset-vnnews', 'repeat': 1, 'eval_base': True, 'pruning_method': 'magnitude', 'pruning_rate': [], 'pruning_layer_num': [1, 2, 3, 4, 6], 'pruning_target': '', 'pruning_data': 'oscarvi', 'pruning_n_samples': 128, 'pruning_rand_data': True, 'pruning_batch_size': 32, 'pruning_avg': None, 'pruning_mag_norm': 'l1', 'pruning_grad_T_order': 1}",,
Base model,21.993011474609375,0.7788572311401367
Layers [12] pruned by magnitude method,22.515533447265625,0.24783539772033691
"Layers [12, 28] pruned by magnitude method",23.6959228515625,0.2399439811706543
"Layers [12, 28, 14] pruned by magnitude method",25.20904541015625,0.23018312454223633
"Layers [12, 28, 14, 29] pruned by magnitude method",28.522705078125,0.22881484031677246
"Layers [12, 28, 14, 29, 27, 26] pruned by magnitude method",40.64422607421875,0.21008610725402832
