\documentclass{uetgraduation}

\begin{document}

\studentname{NGÔ ĐỨC HUY}
\title{XXX}
\documenttype{SUMMER INTERNSHIP REPORT}
\major{Computer Science}
\year{2024}
\supervisor{Dr. Tạ Việt Cường}

% \makecovers 
\makeenglishcovers

% \begin{contentlisting}

%     \tableofcontents
%     \listoffigures
%     \listoftables
    
%     \begin{contentlistingsection}{Các từ viết tắt}
%     \end{contentlistingsection}
% \end{contentlisting}

\begin{preamble}{Acknowledgement}
    I would like to express my sincere gratitude to my supervisor, Dr. Tạ Việt Cường, for his guidance, support, and encouragement throughout the internship. His valuable advices and feedbacks have helped me a lot in completing the research and experiments. \par
    I would also like to the Faculty of Information Technology, University of Engineering and Technology, VNU, and HMI lab for providing me with the opportunity to participate in the summer internship program. \par
\end{preamble}


\begin{preamble}{Abstract}
    \textbf{Abstract:}
    This is the report on the research and experiments on Large Language Models (LLMs) during the summer internship at the HMI Lab, Faculty of Information Technology, University of Engineering and Technology, VNU, under the guidance of Dr. Tạ Việt Cường. \par
    \textit{\textbf{Keywords:} 1, 2}
\end{preamble}

\chapter{Introduction}
\section{Large Language Models and Small Language Models}

Large Language Models (LLMs) are machine learning models that are capable of processing and understanding natural language through learning from a large amount of text data. They can predict, generate text, answer questions, translate, and perform many other complex language tasks. Thanks to the computational power and large-scale data, these models have made significant breakthroughs in Natural Language Processing (NLP). \par 
LLMs can come in various sizes, from a few million to hundreds of billions of parameters. The larger the model, the better the performance, but also the higher the computational cost. For example, GPT-3, one of the largest LLMs, has 175 billion parameters, which requires a large amount of memory and computational resources to train and deploy. \par
Therefore, one of the challenges when using LLMs is the high computational cost and resource requirements. To address this issue, Small Language Models (SLMs) are introduced, which are smaller versions of LLMs that could be aqquired by creating from scratch or through processes such as knowledge distillation, model finetuning, and other training techniques. These SLMs still retain the core language capabilities like other larger model but with lower resource requirements, faster query processing speed, at a small exchanched cost of performance. \par
The research and use of small language models is a new and relatively important research direction in the field of NLP, helping to optimize the performance and cost of NLP applications in practice, or in training step, it may reduce the amount of data or energy required, which is beneficial for the environment. This could be the way for small organizations or individuals lacking resources to enter the field of language models research and development. \par

\section{Research Objectives}
This report presents the research and experiments on a small language model based on PhoGPT, a state of the art LLMs for Vietnamese. The process including explore the process of modifying the model, evaluating the performance of the model in specific evaluations. In overall, the report aims to answer the following questions: \par

\begin{itemize}
    \item Can we modify a LLM like PhoGPT to create a smaller version with fewer parameters and lower resource requirements?
    \item What is the tradeoff between the performance and the resource requirements of the SLM compared to the original LLM?
\end{itemize}

\chapter{Nghiên cứu liên quan}
\section{PhoGPT}
PhoGPT là một series model LLM open-source dành cho Tiếng Việt do VinAIResearch nghiên cứu và phát triển hoàn chỉnh. Trong đó PhoGPT-Chat-4B là model chính với 3.7 tỉ tham số. Model này đã đạt được kết quả tốt trên nhiều bài đánh giá cho tiếng Việt. \\

TABLE here ! \\

\section{Tinh chỉnh mô hình}
Kết quả thực nghiệm trong việc xây dựng mô hình PanGu-$\pi$-1.5B-Pro dành cho tiếng Trung từ Huawei Noah's Ark Lab đã chỉ ra một phương pháp tinh chỉnh mô hình hiệu quả. Mô hình gốc được sử dụng là PanGu-$\pi$-7B với 7 tỉ tham số, sau đó được tinh chỉnh thành mô hình nhỏ hơn với 1.5 tỉ tham số. 


\chapter{Phương pháp nghiên cứu ?}
\section{Research Question 1:}
\section{Research Question 2}

\chapter{Thực nghiệm}
\section{blabla}

\chapter{Kết luận}
\section{blabla}

\end{document}