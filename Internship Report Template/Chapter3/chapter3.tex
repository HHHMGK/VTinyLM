\chapter{Research methodology}
In this chapter, the methods used to conduct the research and answer to the 2 research questions mentioned previously will be presented. Firstly, the method for modifying the model architecture will be discussed. And then the performance metrics and the results of the modified model will be presented.

\section{Model architecture modification}
The model architecture modification is done by changing the model's architecture to a smaller one. One of the most common ways to reduce the size of the model is to reduce the number of layers in the model. The layers in the concept of a generative Transformer LLM is referred to as the number of Transformer blocks placed consecutively in the model. The Transformer block is the basic building block of the Transformer model, which consists of a multi-head self-attention mechanism and a feed-forward neural network. The number of Transformer blocks in a model is the most important factor that determines the model's size. Therefore, reducing the number of Transformer blocks in the model will reduce the model's size significantly. \par 
Since the blocks are placed consecutively in the model, i.e., the output of the previous block is the input of the next block, the process happens layer by layer automatically. Therefore layer reduction can be done by simply removing the layers from the model. \par

\section{Metrics and performance}
The performance of the modified model is evaluated using the perplexity metric. Perplexity is a common metric used to evaluate the performance of a language model. It is calculated as the exponential of the cross-entropy loss of the model on the test dataset. The lower the perplexity, the better the model's performance. \par
The perplexity is calculated as follows:

\begin{equation}
    \text{Perplexity} = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log p(x_i|x_{<i})\right)
\end{equation}
\begin{align*}
    \text{where } &N \text{ is the number of tokens in the sequence} \\
    &p(x_i|x_{1:i-1}) \text{ is the probability of the i-th token } x_i \text{ given the preceding tokens } x_{1:i-1}
\end{align*}

The performance of the model is evaluated by measuring the perplexity of the model on the test dataset consisting of input prompts sequences. The process is done by feeding the input sequences to the model and calculating the perplexity of the model on the output sequences. Then the perplexity is averaged over the whole test dataset to get the final perplexity score of the model. \par
