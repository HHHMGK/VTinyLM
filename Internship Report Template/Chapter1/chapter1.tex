\chapter{Introduction}

\section{Large Language Models and Tiny Language Models}
Large Language Models (LLMs) are machine learning models that are capable of processing and understanding natural language through learning from a large amount of text data. They can predict, generate text, answer questions, translate, and perform many other complex language tasks. Thanks to the computational power and large-scale data, these models have made significant breakthroughs in Natural Language Processing (NLP). \par 
LLMs can come in various sizes, from a few millions to billions or trillions of parameters. The larger the model, the better the performance, but also the higher the computational cost. For example, GPT-4, one of the largest LLMs, has 1.8 trillion parameters, which requires a tremendous amount of memory and computational resources to train and deploy. \par
Therefore, one of the challenges when using LLMs is the high computational cost and resource requirements. To address this issue, Tiny Language Models are introduced, which are smaller versions of LLMs that could be aqquired by creating from scratch or through processes such as knowledge distillation, model finetuning, and other training techniques. These models still retain the core language capabilities like other larger model but with lower resource requirements, faster query processing speed, at a small exchanched cost of performance. \par
The research and use of tiny language models is a new and relatively important research direction in the field of NLP, helping to optimize the performance and cost of NLP applications in practice, or in training step, it may reduce the amount of data or energy required, which is beneficial for the environment. This could be the way for small organizations or individuals lacking resources to enter the field of language models research and development. \par

\section{Research Objectives}
This report presents the research and experiments on a tiny language model created by modifying PhoGPT, a state of the art languege models for Vietnamese. The process including explore the process of modifying the model, evaluating the performance of the model in specific evaluations. In overall, the report aims to answer the following questions: \par

\begin{itemize}
    \item Can we modify a LLM like PhoGPT to create a smaller version with fewer parameters and lower resource requirements?
    \item What is the tradeoff between the performance and the resource requirements of the SLM compared to the original LLM?
\end{itemize}